<!doctype html>
<html lang="en">
 <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>How should I calculate the entropy of a password?</title>
  <link rel="stylesheet" type="text/css" href="/css/base.css">
  <script type="text/x-mathjax-config">
            MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
            TeX: {
            extensions: ["begingroup.js"],
            noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
            Macros: { href: "{}" }
            },
            messageStyle: "none",
            styles: { ".MathJax_Display, .MathJax_Preview, .MathJax_Preview > *": { "background": "inherit" } },
            SEEditor: "mathjaxEditing"
            });
        </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"></script>
 </head>
 <body>
  <div class="question">
   <a href="/questions">All Questions</a>
   <h2>How should I calculate the entropy of a password?</h2>
   <div class="container-metadata">
    <div>
     <span>Score: </span> <span>116</span>
    </div>
    <div>
     <span>Asker: </span> <span>this.josh</span>
    </div>
    <div>
     <span>Asked: </span> <span>11 Aug 2011 at 06:03</span>
    </div>
    <div>
     <a href="https://crypto.stackexchange.com/questions/374/how-should-i-calculate-the-entropy-of-a-password">source</a>
    </div>
   </div>
   <div>
    <p>If part of the password is a whole regular English word, does the entropy of that part depend on the number of English words in existence, the number of English words known by the choosing algorithm, the number of English words assumed by the attacker?</p>
    <p>Does the language matter, is the average entropy per word in German, French, Italian, or Spanish significantly different from the average entropy in English?</p>
    <p>Does a numeric digit always have an entropy of $\log_2(10) = 3.321928$?</p>
   </div>
   <div class="tags">
    <span class="tag">passwords</span><span class="tag">entropy</span>
   </div>
   <hr>
   <div class="comment">
    <table>
     <tbody>
      <tr>
       <td>16</td>
       <td><span>Obligatory XKCD: <a href="http://xkcd.com/936/" rel="nofollow noreferrer">xkcd.com/936</a></span> <span> - </span> <span class="display-name">Tangurena</span> <span> </span> <span class="date">11 Aug 2011 at 22:43</span></td>
      </tr>
      <tr>
       <td>1</td>
       <td><span>You forgot to give a unit for entropy; information theoretically, you'll most likely want "bits" and adding it will correct your question "Does a numeric digit have an entropy of log_2(10) bits?" It does if, and only if, your numeric digit is drawn from a pool of 10 possible digits in a uniformly random manner, which is unusual---i.e. if the number refers to a countable quantity, the leading digit tends to follow a logarithmic, not a uniform distribution.</span> <span> - </span> <span class="display-name">user12600</span> <span> </span> <span class="date">14 Jul 2014 at 08:32</span></td>
      </tr>
      <tr>
       <td></td>
       <td><span>i think of "entropy" as "total number of bruteforce combinations". and clearly this depends on the assumptions of the attacker. the entropy of your password changes according to the assumptions makes about your chosen format.</span> <span> - </span> <span class="display-name">mulllhausen</span> <span> </span> <span class="date">21 Mar 2015 at 13:11</span></td>
      </tr>
      <tr>
       <td></td>
       <td><span><a href="https://github.com/dropbox/zxcvbn" rel="nofollow noreferrer">github.com/dropbox/zxcvbn</a></span> <span> - </span> <span class="display-name">HappyFace</span> <span> </span> <span class="date">17 Feb 2021 at 17:53</span></td>
      </tr>
     </tbody>
    </table>
   </div>
   <br>
   <div>
    <h2 id="answer_1"><span>Answer 1</span> <span class="arrow"> <a href="#answer_2">â†“</a> </span></h2>
    <div class="container-metadata">
     <div>
      <span>Score: </span> <span>108</span>
     </div>
     <div>
      <span>Answerer: </span> <span>Thomas Pornin</span>
     </div>
     <div>
      <span> Answered: </span> <span>11 Aug 2011 at 12:32</span>
     </div>
    </div>
    <div>
     <p>Entropy is a measure of what the password <em>could have been</em> so it does not really relate to the password itself, but to the <strong>selection process</strong>.</p>
     <p>We define the entropy as the value <span class="math-container">$S$</span> such the best guessing attack will require, on average, <span class="math-container">$S/2$</span> guesses. "Average" here is an important word. We assume that the "best attacker" knows all about what passwords are more probable to be chosen than others, and will do his guessing attack by beginning with the most probable passwords. The model is the following: we suppose that the password is generated with a program on a computer; the program is purely deterministic and uses a cryptographically strong PRNG as source of alea (e.g. <code>/dev/urandom</code> on a Linux system, or <code>CryptGenRandom()</code> on Windows). The attacker has a copy of the source code of the program; what the attacker does <em>not</em> have is a copy of the random bits that the PRNG actually produced.</p>
     <p>Entropy is easy to compute if the random parts of the selection process are uniform (e.g. with dice or a computer with a good PRNG -- as opposed to a human being making a "random" chance in his head). For instance, if you have a list of 2000 words and choose one among them (uniformly), then the entropy is <span class="math-container">$S = 2000$</span>. Entropy is often expressed in bits: an entropy of <span class="math-container">$n$</span> bits is what you get out of a sequence of <span class="math-container">$n$</span> bits which have been selected uniformly and independently of each other (e.g. by flipping a coin for each bit); it is a simple logarithmic scale: "<span class="math-container">$n$</span> bits of entropy" means "entropy is <span class="math-container">$S = 2^n$</span>" (and the attack cost is then <span class="math-container">$2^{n-1}$</span> on average).</p>
     <p>If you think of a password as two halves chosen independently of each other, then the total entropy is the product of the entropies of each half; when expressed with bits, this becomes a sum, because that's what logarithms do: they transform multiplications into sums. So if you take two words, randomly and independently (i.e. never ruling out any <em>combination</em>, even if the two words turn out to be the same), out of a list of 2000, then the total entropy is <span class="math-container">$2000\cdot2000 = 4000000$</span>. Expressed in bits, each word implies an entropy of about 11 bits (because <span class="math-container">$2^{11}$</span> is close to <span class="math-container">$2000$</span>), and the total entropy is close to 22 bits (and, indeed, <span class="math-container">$2^{22}$</span> is close to <span class="math-container">$4000000$</span>).</p>
     <p>This answers your question about digits: a decimal digit has entropy 10, as long as it is chosen randomly and uniformly and independently from all other random parts of the password. Since <span class="math-container">$10 = 2^{3.321928...}$</span> then each digit adds about 3.32 extra bits to the entropy.</p>
     <p>If a human being is involved in the selection process then calculating the entropy becomes much more difficult. For instance, if a human chooses two digits and the first digit is '4', then the probability that the second digit is '2' is quite higher than <span class="math-container">$\frac1{10}$</span>. It could be argued that it is also difficult <em>for the attacker</em>: he will also have more work to do to sort the potential passwords so that he begins with the most probable. But this becomes a psychological problem, where the attacker tries to model the thinking process of the user, and we try to model the thinking process of the attacker: it will be hard to quantify things with any decent precision.</p>
    </div>
    <hr>
    <div class="comment">
     <table>
      <tbody>
       <tr>
        <td>2</td>
        <td><span>So you're saying position of a symbol has no effect on the amount of entropy the symbol contains?</span> <span> - </span> <span class="display-name">this.josh</span> <span> </span> <span class="date">11 Aug 2011 at 21:26</span></td>
       </tr>
       <tr>
        <td>14</td>
        <td><span>A symbol contains no entropy by itself: entropy is a property of the way the symbol was generated -- and that generation process produces an <i>ordered</i> sequence of symbols. Position is part of what is generated.</span> <span> - </span> <span class="display-name">Thomas Pornin</span> <span> </span> <span class="date">11 Aug 2011 at 21:29</span></td>
       </tr>
       <tr>
        <td>31</td>
        <td><span>Perhaps you could put it like this: entropy exists from the perspective of some party. To the user who knows their password, the password has zero entropy. But usually the party we're discussing the entropy is some hypothetical attacker. If you were to show him the password one symbol at a time, how much new information (in units of bits) would he learn as he sees each one arrive in turn? So the entropy is about the generation of the password insofar as we expect the attacker to know the method, but not the values, of that generation process.</span> <span> - </span> <span class="display-name">Marsh Ray</span> <span> </span> <span class="date">11 Aug 2011 at 22:02</span></td>
       </tr>
       <tr>
        <td></td>
        <td><span>Entropy is defined for random variable. It would be great to re-state the above write-up in terms of random variable.</span> <span> - </span> <span class="display-name">user1700890</span> <span> </span> <span class="date">20 Sep 2015 at 20:22</span></td>
       </tr>
       <tr>
        <td></td>
        <td><span>@ThomasPornin As a follow up, if an attacker KNEW that the password was an all lowercase English word, then could you say that the entropy is actually much less? Could we even calculate entropy in that case, where we are roughly looking at 1 in 1 Million words rather than individual characters?</span> <span> - </span> <span class="display-name">Chiramisu</span> <span> </span> <span class="date">11 Sep 2017 at 16:56</span></td>
       </tr>
       <tr>
        <td>2</td>
        <td><span>@Chiramisu There are in fact about 171476 words in current English (at least so say the people who maintain the Oxford English Dictionary). Assuming that the target selected one of these words with perfectly uniform randomness, then the entropy is 171476, i.e. about 17.39 bits. In practice, human users would discard words that they cannot remember the spelling of, so the actual entropy would probably be lower. Entropy <i>cannot</i> be larger, though.</span> <span> - </span> <span class="display-name">Thomas Pornin</span> <span> </span> <span class="date">11 Sep 2017 at 17:35</span></td>
       </tr>
       <tr>
        <td>1</td>
        <td><span>@ThomasPornin Ah, indeed! I was going for best case including all scientific, medical, legal, slang, obsolete, etc, but no worries. By that math, then even with a million words there are still only about 19.93 bits of entropy. However, by using at least three Oxford English words we can increase this to ~69.55 bits and get pretty good security taking about 27.4 years to guess at 1 Trillion guesses per second. Not bad for strictly lowercase :) ... or 10 days at 1 Quadrillion guesses per second. &gt;.&lt;</span> <span> - </span> <span class="display-name">Chiramisu</span> <span> </span> <span class="date">11 Sep 2017 at 18:25</span></td>
       </tr>
       <tr>
        <td></td>
        <td><span>If I <code>cat /dev/urandom | head -c 16 &gt; 16byteSecretKey</code>, would the entropy of <code>16byteSecretKey</code> be 128 bits?</span> <span> - </span> <span class="display-name">No_name</span> <span> </span> <span class="date">25 Jan 2021 at 16:17</span></td>
       </tr>
      </tbody>
     </table>
    </div>
   </div>
   <div>
    <h2 id="answer_2"><span>Answer 2</span> <span class="arrow"> <a href="#answer_1">â†‘</a> </span> <span class="arrow"> <a href="#answer_3">â†“</a> </span></h2>
    <div class="container-metadata">
     <div>
      <span>Score: </span> <span>14</span>
     </div>
     <div>
      <span>Answerer: </span> <span>Paulo Marques</span>
     </div>
     <div>
      <span> Answered: </span> <span>11 Aug 2011 at 12:00</span>
     </div>
    </div>
    <div>
     <p>Information entropy is closely related to the "predictability" of the same information.</p>
     <p>When we talk about password entropy we are usually concerned with how easy it is for a password cracking software to predict a password. The more passwords the software has to try before guessing the password the larger the entropy is.</p>
     <p>You can check software like John the Ripper (http://www.openwall.com/john/). It's free and you can download for free a list of words from 20 different languages (to answer your question about different languages).</p>
     <p>Using this entropy concept, its easy to see that a digit in the middle of a word probably has more entropy than a digit in the end of a word. John will try words + 1~2 digits combinations pretty early in the attempts, so something like crypto5 has less entropy than cryp5to and uses the same characters.</p>
    </div>
    <hr>
    <div class="comment">
     <table>
      <tbody>
       <tr>
        <td></td>
        <td><span>So position of a symbol impacts the entropy of the symbol based on the likelyness of an attacker to attempt that class of symbols in that position?</span> <span> - </span> <span class="display-name">this.josh</span> <span> </span> <span class="date">11 Aug 2011 at 20:52</span></td>
       </tr>
       <tr>
        <td></td>
        <td><span>@this.josh If you expect the attacker to expect you to use some positions more than others, yes. Whether or not that's a good expectation is an interesting discussion. There is said to be a government agency using a password restriction of "if there is only one letter or special character, it should not be either the first or last character". I wrote a blog post a while back on this <a href="http://extendedsubset.com/?p=18" rel="nofollow noreferrer">extendedsubset.com/?p=18</a></span> <span> - </span> <span class="display-name">Marsh Ray</span> <span> </span> <span class="date">11 Aug 2011 at 22:11</span></td>
       </tr>
      </tbody>
     </table>
    </div>
   </div>
   <div>
    <h2 id="answer_3"><span>Answer 3</span> <span class="arrow"> <a href="#answer_2">â†‘</a> </span> <span class="arrow"> <a href="#answer_4">â†“</a> </span></h2>
    <div class="container-metadata">
     <div>
      <span>Score: </span> <span>4</span>
     </div>
     <div>
      <span>Answerer: </span> <span>John Whitermaker</span>
     </div>
     <div>
      <span> Answered: </span> <span>24 Feb 2012 at 21:35</span>
     </div>
    </div>
    <div>
     <p>Basically, any password is a string of letters and entropy can be easily calculated. For example you can use <a href="http://www.shannonentropy.netmark.pl" rel="nofollow">Shannon entropy calculator</a> or by hand using a scientific calculator.</p>
     <p>Entropy is calculated based on frequencies of letters in the password, it does not care about used language. So diverse passwords with many different letters are preferred as entropy will be larger. Words are treated equally if they have the same proportions of used letters, e.g. English 'and' and Indonesian 'dan' has the same entropy). This means, contrary to what Paulo said earlier, that 'cryp5to' and 'crypto5' has the same entropy, entropy does not care about letter order. If you do not believe this, try it yourself by entering similar examples into <a href="http://www.shannonentropy.netmark.pl" rel="nofollow">http://www.shannonentropy.netmark.pl</a></p>
     <p>Of course, if an attacker will assume that your password is a word, not a random string (most people do that) he will use a dictionary to break your password and he will break it earlier, but his knowledge that you use a word, not a random string is actually information which decreases entropy, so he used external information to lower the entropy needed to break it.</p>
     <p>"Does the entropy of that part depend on the number of English words in existence, ..." NO, it depends on all the combinations which can be done based on password length and diversity.</p>
     <p>"... the number of English words known by the choosing algorithm..." it may affect the algorithm, but not from an entropy point of view, e.g. if this algorithm will be: just try all words from dictionary in which there is no crypto5, but crypto is present, it fails, but if the algorithm is more clever, for instance take all words from dictionary and mutate them by random letter or number it will finally find crypto5.</p>
     <p>" ... the number of English words assumed by the attacker?" it may affect the algorithm, but not from an entropy point of view, see above, and remember you do not know who and how will hack your password, so you cannot assume anything like I will use different language, because it has more words, but on the other hand you can use different language if it has more letters (and you will use them in the password).</p>
     <p>"Does the language matter, is the average entropy per word in German, French, Italian, or Spanish significantly different from the average entropy in English?" You can calculate entropy for different languages (actually this is what Shannon did), but again it does not influence the entropy of the password.</p>
     <p>"Does a numeric digit always have an entropy of $\log_2(10) = 3.321928$?" No, base 2 is the most common, and it has nothing to numeric digits, it can be used also to letters or any other signs, see Wikipedia [information theory entropy]</p>
    </div>
    <hr>
    <div class="comment">
     <table>
      <tbody>
       <tr>
        <td>7</td>
        <td><span>The context for the question is passwords in the face of attacks, as covered very well by Thomas' answer. As noted in <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="nofollow noreferrer">Entropy (information theory)</a>, entropy is defined in the context of a probabilistic model. Clearly we should expect that an attacker is going to use a smart probabilistic model, so it is indeed the case that English words have less useful entropy in a password context than the same letters in a different order. We need to use more than the original "Shannon entropy", which has a different set of assumptions.</span> <span> - </span> <span class="display-name">nealmcb</span> <span> </span> <span class="date">16 Dec 2014 at 18:11</span></td>
       </tr>
      </tbody>
     </table>
    </div>
   </div>
   <div>
    <h2 id="answer_4"><span>Answer 4</span> <span class="arrow"> <a href="#answer_3">â†‘</a> </span></h2>
    <div class="container-metadata">
     <div>
      <span>Score: </span> <span>1</span>
     </div>
     <div>
      <span>Answerer: </span> <span>Steven Hatzakis</span>
     </div>
     <div>
      <span> Answered: </span> <span>24 Sep 2018 at 16:01</span>
     </div>
    </div>
    <div>
     <p>The entropy for a randomly generated password is based on the character <code>Library</code> space (i.e. range of valid characters) and then the <code>length</code> of the passwords (i.e. total number of characters in the password), and with no other constraints (i.e. ability to have a random message that produces a password of all the same characters even if it is unlikely for that to occur).</p>
     <p>In such a setup, the entropy will be the <span class="math-container">$log_2{(Library^{length}}$</span>), see below for examples and <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="nofollow noreferrer">Claude Shannon's formula</a>.</p>
     <p>The entropy "H" of a discrete random variable "X" is defined as:</p>
     <p><span class="math-container">${\\H(X) = - \sum_{i=1}^{n} P(x_i) \ log_b P(x_i) }$</span></p>
     <p>If the English word is a mnemonic and represents some underlying index value or other code value such as ASCII or UTF-8, then I don't think there is a difference so long as it was chosen randomly, as its entropy will depend entirely on the range of words or letters that it was chosen from. There is a difference though between the user choosing a word, versus randomly chosen letters that "happen" to equal a word when read from left to right etc..</p>
     <p>Here is a simple explanation regarding password entropy, and depending on what needs to be measured. Let's first assume two following two points:</p>
     <ol>
      <li>The password has a specific "<strong>length</strong>" (consisting of its number of characters, some of - or all of which - may be duplicate/identical and/or repeat consecutively).</li>
      <li>Any character in the password has been chosen from a single common library or "<strong>range</strong>" of unique characters and chosen randomly using a cryptographically secure process.</li>
     </ol>
     <p>Formula:</p>
     <ul>
      <li>
       <p>Log2(Possible combinations)= <strong>overall password entropy</strong></p></li>
      <li>
       <p>Range^Length=<strong>Possible combinations</strong> (can also be rounded as 2^overall password entropy)</p></li>
      <li>Log2(Range) = <strong>Entropy per character</strong></li>
      <li>Entropy per character * Length = overall password entropy</li>
     </ul>
     <p>Example Test:</p>
     <ul>
      <li>Range = 2048 unique character values (or 2048 unique words)</li>
      <li>Length =12 characters (or 12 words, some or all of which may repeat)</li>
      <li>Possibilities = 5444517870735015415413993718908291383296 or 2048^12</li>
      <li>Overall Entropy = 132 or log2(possibilities)</li>
      <li>Entropy per character (or per word if words are used) = 11 or log2(2048)</li>
     </ul>
     <p>Another way to double-check roughly (depending on precision available if dealing with decimals and not whole number results): <strong>2^(log2(Range)*Length) == (2^Entropy)</strong></p>
     <p>In Python3: <strong><code>2**(int(math.log2(2048))*12) == int(2**132)</code></strong></p>
     <hr>
     <p>P.S. I think frequency analysis is useful here in two situations, one) the password was chosen deterministically without a crypto-secure process, and/or two) the characters in the library are either not distinctly unique (i.e. one or more duplicates exist, or many characters share strong similarities) or other unknown leakages of information in the library set.</p>
    </div>
    <hr>
    <div class="comment">
     <table>
      <tbody>
       <tr>
        <td></td>
        <td><span>Nice math but misses the point for passwords which as <b>not</b> random sequences. Consider <a href="https://github.com/danielmiessler/SecLists/tree/master/Passwords" rel="nofollow noreferrer">frequent passwords lists</a>. ordered by usage frequency. If the password is toward the front of the list is is not secure al all. Passwords are cracked using such lists in most instances.</span> <span> - </span> <span class="display-name">zaph</span> <span> </span> <span class="date">24 Sep 2018 at 16:12</span></td>
       </tr>
       <tr>
        <td>1</td>
        <td><span>I was addressing the question and more important point, which is - now more than ever it's critical to start using random passwords, and knowing the entropy for a given password will entirely depend on its length and the range of characters available in the library used. Even if you choose a password yourself, there are ways to make it more secure by first being aware of how entropy is calculated.</span> <span> - </span> <span class="display-name">Steven Hatzakis</span> <span> </span> <span class="date">24 Sep 2018 at 16:17</span></td>
       </tr>
       <tr>
        <td></td>
        <td><span>See <a href="http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63b.pdf" rel="nofollow noreferrer">NIST Digital Identity Guidelines</a>. Also passwords created by humans are rarely random.</span> <span> - </span> <span class="display-name">zaph</span> <span> </span> <span class="date">24 Sep 2018 at 16:19</span></td>
       </tr>
       <tr>
        <td>1</td>
        <td><span>I was just looking at the link a few days ago! I agree humans cannot choose randomly to the degree a cryptographically-secure process can (even hitting the keyboard randomly is not entirely random when repeated). But there are still steps one can take, such as in the NIST guidelines, along with two-factor authentication and mixing salts and sourcing entropy from various methods so that if one is corrupt the password is still secured by the other entropy inputs (among many other proven and new methods).</span> <span> - </span> <span class="display-name">Steven Hatzakis</span> <span> </span> <span class="date">24 Sep 2018 at 16:29</span></td>
       </tr>
      </tbody>
     </table>
    </div>
   </div>
   <br>
   <div>
    <span>License: </span> <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> <span>by </span> <a href="https://stackoverflow.com/legal/terms-of-service#licensing">Stack Overflow Inc.</a>
   </div>
  </div>
 </body>
</html>