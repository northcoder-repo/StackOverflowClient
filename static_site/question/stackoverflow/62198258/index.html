<!doctype html>
<html lang="en">
 <head>
  <meta charset="UTF-8">
  <title>why is nothing getting parsed in my web scraping program?</title>
  <link rel="stylesheet" type="text/css" href="/css/base.css">
  <script type="text/x-mathjax-config">
            MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
            TeX: {
            extensions: ["begingroup.js"],
            noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
            Macros: { href: "{}" }
            },
            messageStyle: "none",
            styles: { ".MathJax_Display, .MathJax_Preview, .MathJax_Preview > *": { "background": "inherit" } },
            SEEditor: "mathjaxEditing"
            });
        </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"></script>
 </head>
 <body>
  <div class="question">
   <a href="/questions">All Questions</a>
   <h2>why is nothing getting parsed in my web scraping program?</h2>
   <div class="container-metadata">
    <div>
     <span>Score: </span> <span>1</span>
    </div>
    <div>
     <span>Asker: </span> <span>user9387061</span>
    </div>
    <div>
     <span>Asked: </span> <span>4 Jun 2020 at 15:22</span>
    </div>
    <div>
     <a href="https://stackoverflow.com/questions/62198258/why-is-nothing-getting-parsed-in-my-web-scraping-program">source</a>
    </div>
   </div>
   <div>
    <p>I made this code to search all the top links in google search. But its returning none.</p>
    <pre><code>import webbrowser, requests
from bs4 import BeautifulSoup
string = 'selena+gomez'
website = f'http://google.com/search?q={string}'
req_web = requests.get(website).text
parser = BeautifulSoup(req_web, 'html.parser')
gotolink = parser.find('div', class_='r').a["href"]
print(gotolink)
</code></pre>
   </div>
   <div class="tags">
    <span class="tag">web-scraping</span><span class="tag">beautifulsoup</span><span class="tag">python-requests</span><span class="tag">python-webbrowser</span>
   </div>
   <hr>
   <div class="comment">
    <table>
    </table>
   </div>
   <br>
   <div>
    <h2 id="answer_1"><span>Answer 1</span> <span class="arrow"> <a href="#answer_2">↓</a> </span></h2>
    <div class="container-metadata">
     <div>
      <span>Score: </span> <span>1</span>
     </div>
     <div>
      <span>Answerer: </span> <span>Andrej Kesely</span>
     </div>
     <div>
      <span> Answered: </span> <span>4 Jun 2020 at 18:06</span>
     </div>
    </div>
    <div>
     <p>Google needs that you specify <code>User-Agent</code> http header to return correct page. Without the correct <code>User-Agent</code> specified, Google returns page that doesn't contain <code>&lt;div&gt;</code> tags with <code>r</code> class. You can see it when you do <code>print(soup)</code> with and without <code>User-Agent</code>.</p>
     <p>For example:</p>
     <pre><code>import requests
from bs4 import BeautifulSoup

string = 'selena+gomez'
headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:76.0) Gecko/20100101 Firefox/76.0'}
website = f'http://google.com/search?hl=en&amp;q={string}'

req_web = requests.get(website, headers=headers).text
parser = BeautifulSoup(req_web, 'html.parser')
gotolink = parser.find('div', class_='r').a["href"]
print(gotolink)
</code></pre>
     <p>Prints:</p>
     <pre><code>https://www.instagram.com/selenagomez/?hl=en
</code></pre>
    </div>
    <hr>
    <div class="comment">
     <table>
      <tbody>
       <tr>
        <td></td>
        <td><span>This is very helpful (to me). Would you be able to add a sentence about how this solves the problem?</span> <span> - </span> <span class="display-name">andrewJames</span> <span> </span> <span class="date">4 Jun 2020 at 18:16</span></td>
       </tr>
       <tr>
        <td></td>
        <td><span>@andrewjames I added some explanation. It boils down, that without <code>User-Agent</code> Goggle returns other version of HTML that you see in browser.</span> <span> - </span> <span class="display-name">Andrej Kesely</span> <span> </span> <span class="date">4 Jun 2020 at 18:19</span></td>
       </tr>
       <tr>
        <td>1</td>
        <td><span>@AndrejKesely thank you very much bro!!!! That solved my problem...</span> <span> - </span> <span class="display-name">user9387061</span> <span> </span> <span class="date">5 Jun 2020 at 17:06</span></td>
       </tr>
      </tbody>
     </table>
    </div>
   </div>
   <div>
    <h2 id="answer_2"><span>Answer 2</span> <span class="arrow"> <a href="#answer_1">↑</a> </span></h2>
    <div class="container-metadata">
     <div>
      <span>Score: </span> <span>0</span>
     </div>
     <div>
      <span>Answerer: </span> <span>Dmitriy Zub</span>
     </div>
     <div>
      <span> Answered: </span> <span>30 Aug 2021 at 06:49</span>
     </div>
    </div>
    <div>
     <p>Answer from <a href="https://stackoverflow.com/a/62201346/15164646">Andrej Kesely</a> will throw an error since this <code>css</code> class no longer exists:</p>
     <pre class="lang-py prettyprint-override"><code>gotolink = parser.find('div', class_='r').a["href"]
AttributeError: 'NoneType' object has no attribute 'a'
</code></pre>
     <hr>
     <p>Learn more about <a href="https://developer.mozilla.org/en-US/docs/Glossary/User_agent" rel="nofollow noreferrer"><code>user-agent</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Glossary/Request_header" rel="nofollow noreferrer">request headers</a>.</p>
     <p>Basically <code>user-agent</code> let identifies the browser, its version number, and its host operating system that representing a person (<strong>browser</strong>) in a Web context that lets servers and network peers identify if it's a bot or not.</p>
     <p>In this case, you need to send a fake <code>user-agent</code> so Google would treat your request as a "real" user visit, <a href="https://developer.mozilla.org/en-US/docs/Glossary/User_agent" rel="nofollow noreferrer">also known as <em><code>user-agent</code> spoofing</em></a>.</p>
     <p>Pass <code>user-agent</code> in request <code>headers</code>:</p>
     <pre class="lang-py prettyprint-override"><code>headers = {
    'User-agent':
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582"
}

requests.get(YOUR_URL, headers=headers)
</code></pre>
     <hr>
     <p>Code and <a href="https://replit.com/@DimitryZub1/Google-Scrape-Title-Description-Linkpythonbs4#main.py" rel="nofollow noreferrer">example in the online IDE</a>:</p>
     <pre class="lang-py prettyprint-override"><code>from bs4 import BeautifulSoup
import requests

headers = {
    'User-agent':
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582"
}

params = {
  "q": "selena gomez"
}

html = requests.get('https://www.google.com/search', headers=headers, params=params)
soup = BeautifulSoup(html.text, 'lxml')

link = result.select_one('.yuRUbf a')['href']
print(link)

# https://www.instagram.com/selenagomez/
</code></pre>
     <hr>
     <p>Alternatively, you can achieve the same thing by using <a href="https://serpapi.com/organic-results" rel="nofollow noreferrer">Google Organic Results API</a> from SerpApi. It's a paid API with a free plan.</p>
     <p>Essentially, the main difference in your case is that you don't need to think about how to bypass Google blocks if they appear or figure out how to scrape elements that are a bit harder to scrape since it's already done for the end-user. The only thing that needs to be done is just get the data you want from the JSON string.</p>
     <p>Example code:</p>
     <pre class="lang-py prettyprint-override"><code>import os
from serpapi import GoogleSearch

params = {
    "engine": "google",
    "q": "selena gomez",
    "api_key": os.getenv("API_KEY"),
}

search = GoogleSearch(params)
results = search.get_dict()

# [0] means index of the first organic result 
link = results['organic_results'][0]['link']
print(link)

# https://www.instagram.com/selenagomez/
</code></pre>
     <blockquote>
      <p>Disclaimer, I work for SerpApi.</p>
     </blockquote>
    </div>
    <hr>
    <div class="comment">
     <table>
     </table>
    </div>
   </div>
   <br>
   <div>
    <span>License: </span> <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> <span>by </span> <a href="https://stackoverflow.com/legal/terms-of-service#licensing">Stack Overflow Inc.</a>
   </div>
  </div>
 </body>
</html>